{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is the old version of ***Current Constructing Cloud Perimeter (with tensors).ipynb*** which was extensivley error checked. This notebook performs the same functionality as ***Current Constructing Cloud Perimeter (with tensors).ipynb*** but uses numpy and was hence too slow to be run on all 129 files."
      ],
      "metadata": {
        "id": "sYRaADolvzek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeFOsbwGf7oZ"
      },
      "outputs": [],
      "source": [
        "from netCDF4 import Dataset\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from mpl_toolkits import mplot3d\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import warnings\n",
        "from csv import writer\n",
        "from scipy import ndimage as nd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PsfSB3ug0FY",
        "outputId": "2bc90e8e-1d34-4dd0-9b85-674857233e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4JgADVeg_ta"
      },
      "outputs": [],
      "source": [
        "#working perimeter computer (with corrected padding)\n",
        "def file_generator_padding():\n",
        "      #initialise a counter for the number of files considered\n",
        "      counter=0\n",
        "      #store the path to the COGS files as a variable\n",
        "      directory = \"/content/gdrive/MyDrive/Colab Notebooks/Data/Bath/COGS/\"\n",
        "      #store the file path for the list of all COGS files in chronological order\n",
        "      file_list_csv = \"/content/gdrive/MyDrive/Colab Notebooks/Data/processing_file_list_after23.csv\"\n",
        "      #use pandas to read in the file list csv file and iterate through them\n",
        "      df = pd.read_csv(file_list_csv)\n",
        "      for row_index, filename in df.iterrows():\n",
        "        #initialise a counter which counts the number of times a missing value occurs in a \n",
        "        #cell and that cell is therefore discounted  \n",
        "        missing_value_counter=0\n",
        "        #initialise the vectors which will store our variables cloud perimeter, cloud fraction\n",
        "        #and height, these vectors will be the response and explanatory variables for the cells,\n",
        "        #respectively\n",
        "        cloud_perimeter_vector=[]\n",
        "        cloud_fraction_vector=[]\n",
        "        height_vector=[]\n",
        "        perimeter_values_vector=[]\n",
        "        #initialise the vector which stores the dimensions of the datasets\n",
        "        dimensions=[]\n",
        "        #print each file name as an error check and to monitor progress of the run\n",
        "        print(filename['File_names'])\n",
        "        #access each file name in the list using the key 'File_names'\n",
        "        filename_str=str(filename['File_names'])\n",
        "        #store the path to the current file as a variable\n",
        "        file_path=directory+filename_str[1:-2]\n",
        "        #add one to the counter as a new file has been accessed\n",
        "        counter+=1\n",
        "        #print the file path to check it has been formatted correctly\n",
        "        print(file_path)\n",
        "        #read in the netCDF data as a Dataset\n",
        "        cloud_data_set = Dataset(file_path, mode='r')\n",
        "        #suppress the warnings that arise when casting as a numpy array\n",
        "        warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "        #store the different cloud variables as numpy arrays\n",
        "        x = np.array(cloud_data_set.variables['x'])\n",
        "        y = np.array(cloud_data_set.variables['y'])\n",
        "        z = np.array(cloud_data_set.variables['z'])\n",
        "        cloud = np.array(cloud_data_set.variables['cloud'])\n",
        "        time = np.array(cloud_data_set.variables['time'])\n",
        "        #turn deprecation warnings back on\n",
        "        warnings.filterwarnings(\"default\", category=DeprecationWarning)\n",
        "        #append the dimensions of the dataset i.e. the total possible cloud \n",
        "        #perimeter values to the end of the dimensions vector\n",
        "        possible_values=np.shape(cloud)[0]*20*4*4\n",
        "        #run through all time step in the file\n",
        "        for t in range(cloud.shape[0]):\n",
        "          #x and y dimensions have length 121, so only consider the final 120\n",
        "          #values to make the cloud domain a cube\n",
        "          cloud_cube=cloud\n",
        "          cloud_slice=cloud_cube[t,:,:,:]\n",
        "          perimeter_values_counter=0\n",
        "          #specify the kernel which allows the computation of the cloud perimeter\n",
        "          k=[[[0,0,0],[0,1,0],[0,0,0]],[[0,1,0],[1,0,1],[0,1,0]],[[0,0,0],[0,1,0],[0,0,0]]]\n",
        "          #cast the cloud array and kernels as Tensorflow tensors\n",
        "          cloud_tensor = tf.constant(cloud_slice, tf.float32)\n",
        "          k_tensor = tf.constant(k, tf.float32)\n",
        "          #perform the convolution operation to find an array showing the number of neighbouring 1s for each element\n",
        "          kernel_cloud = tf.nn.convolution(tf.reshape(cloud_tensor, [1, 120, 121, 121, 1]), tf.reshape(k_tensor, [3, 3, 3, 1, 1]), padding='VALID')\n",
        "          #subtract this array from 6 in every entry to find the perimeter of each element\n",
        "          kernel_cloud_subtracted = tf.subtract(6,kernel_cloud)\n",
        "          #remove the outer edges of the cloud array so that we can multiply by the convolved array entry-wise\n",
        "          #edges are removed because VALID padding is used\n",
        "          cloud_tensor_reduced=cloud_tensor[1:-1,1:-1,1:-1]\n",
        "          #multiply the two arrays entry-wise so that we only consider the perimeter of elements where a cloud i.e. a 1\n",
        "          #exists (where a 0 exists, this entry will just be zeroed by the pointwise operation)\n",
        "          cloud_perimeter_tensor=tf.math.multiply(cloud_tensor_reduced,tf.squeeze(kernel_cloud_subtracted))\n",
        "          #loop over all 320 cells in the domain\n",
        "          for kk in range(20):\n",
        "              for jj in range(4):\n",
        "                  for ii in range(4):\n",
        "                      if (kk==19 and jj==3 and ii==3):\n",
        "                          #when we lie in the final cell we overlap by as much as necessary\n",
        "                          cloud_cell=cloud_tensor_reduced[-6:,-30:,-30:]\n",
        "                          cloud_perimeter_cell=cloud_perimeter_tensor[-6:,-30:,-30:]\n",
        "                      elif (kk==19 and jj!=3 and ii!=3):\n",
        "                          #when we lie in the final cell we overlap by as much as necessary\n",
        "                          cloud_cell=cloud_tensor_reduced[-6:,30*(jj):30*(jj+1),30*(ii):30*(ii+1)]\n",
        "                          cloud_perimeter_cell=cloud_perimeter_tensor[-6:,30*(jj):30*(jj+1),30*(ii):30*(ii+1)]\n",
        "                      elif (kk==19 and jj==3 and ii!=3):\n",
        "                          #when we lie in the final cell we overlap by as much as necessary\n",
        "                          cloud_cell=cloud_tensor_reduced[-6:,-30:,30*(ii):30*(ii+1)]\n",
        "                          cloud_perimeter_cell=cloud_perimeter_tensor[-6:,-30:,30*(ii):30*(ii+1)]\n",
        "                      elif (kk==19 and jj!=3 and ii==3):\n",
        "                          #when we lie in the final cell we overlap by as much as necessary\n",
        "                          cloud_cell=cloud_tensor_reduced[-6:,30*(jj):30*(jj+1),-30:]\n",
        "                          cloud_perimeter_cell=cloud_perimeter_tensor[-6:,30*(jj):30*(jj+1),-30:]\n",
        "                      elif (kk!=19 and jj!=3 and ii==3):\n",
        "                          #when we lie in the final cell we overlap by as much as necessary\n",
        "                          cloud_cell=cloud_tensor_reduced[6*kk:6*(kk+1),30*(jj):30*(jj+1),-30:]\n",
        "                          cloud_perimeter_cell=cloud_perimeter_tensor[6*kk:6*(kk+1),30*(jj):30*(jj+1),-30:]\n",
        "                      elif (kk!=19 and jj==3 and ii==3):\n",
        "                          #when we lie in the final cell we overlap by as much as necessary\n",
        "                          cloud_cell=cloud_tensor_reduced[6*kk:6*(kk+1),-30:,-30:]\n",
        "                          cloud_perimeter_cell=cloud_perimeter_tensor[6*kk:6*(kk+1),-30:,-30:]\n",
        "                      elif (kk!=19 and jj==3 and ii!=3):\n",
        "                          #when we lie in the final cell we overlap by as much as necessary\n",
        "                          cloud_cell=cloud_tensor_reduced[6*kk:6*(kk+1),-30:,30*(ii):30*(ii+1)]\n",
        "                          cloud_perimeter_cell=cloud_perimeter_tensor[6*kk:6*(kk+1),-30:,30*(ii):30*(ii+1)]\n",
        "                      else:\n",
        "                          #specify which of the interior 320 cells that we lie in\n",
        "                          cloud_cell=cloud_tensor_reduced[6*kk:6*(kk+1),30*(jj):30*(jj+1),30*(ii):30*(ii+1)]\n",
        "                          #specify the equivalent cell for the cloud_perimeter_tensor generated above\n",
        "                          cloud_perimeter_cell=cloud_perimeter_tensor[6*kk:6*(kk+1),30*(jj):30*(jj+1),30*(ii):30*(ii+1)]\n",
        "                      #if any value within the cell is -1 we discount the cell entirely,\n",
        "                      #as prescribed by the methodology\n",
        "                      if np.any(cloud_cell < 0):\n",
        "                          #increment the counter by 1 if we ignore a cell\n",
        "                          missing_value_counter+=1\n",
        "                          continue\n",
        "                      else:\n",
        "                          perimeter_values_counter+=1\n",
        "                          #compute the cloud fraction - divide the number of 1s by the volume of a cell\n",
        "                          cloud_fraction_value=(np.sum(cloud_cell))/(np.shape(cloud_cell)[0]*np.shape(cloud_cell)[1]*np.shape(cloud_cell)[2])\n",
        "                          cloud_fraction_value=float(cloud_fraction_value)\n",
        "                          #append the computed value onto the cloud fraction vector \n",
        "                          cloud_fraction_vector.append(cloud_fraction_value)\n",
        "                          #sum over the entire 3D array to find the total perimeter - a single scalar\n",
        "                          cloud_perimeter_value=tf.reduce_sum(cloud_perimeter_cell)\n",
        "                          cloud_perimeter_value=int(cloud_perimeter_value)\n",
        "                          #normalise the cloud perimeter value by dividing it by the maximum perimeter for a cell (16200)\n",
        "                          cloud_perimeter_value=float(cloud_perimeter_value/16200)\n",
        "                          #append the cloud perimeter value for this time step in this cell to vector\n",
        "                          cloud_perimeter_vector.append(cloud_perimeter_value)\n",
        "\n",
        "                          #append the height to the height vector\n",
        "                          height_vector.append(kk)\n",
        "          #for each time step append the number of cells in which a perimeter value was\n",
        "          #recorded (this will be used in the sonde data function below)                \n",
        "          perimeter_values_vector.append(perimeter_values_counter)\n",
        "\n",
        "        #write the lists to numpy arrays\n",
        "        cloud_perimeter_vector=np.array(cloud_perimeter_vector)\n",
        "        cloud_fraction_vector=np.array(cloud_fraction_vector)\n",
        "        height_vector=np.array(height_vector)\n",
        "        perimeter_values_vector=np.array(perimeter_values_vector)\n",
        "        #print lines to check that the total number of cells for which a perimeter value was recorded\n",
        "        #plus the number of cells discounted due to a missing value equals the total possible cells\n",
        "        print(\"Sum of cloud perimeter cells=\",np.sum(perimeter_values_vector))\n",
        "        print(\"Missing value cells=\",missing_value_counter)\n",
        "        print(\"Total possible cells=\",cloud.shape[0]*20*4*4)\n",
        "        #save the arrays to files on google drive labelled by the name of the COGS file\n",
        "        new_file_path_perimeter=\"/content/gdrive/MyDrive/Colab Notebooks/Data/Variables/Cloud Perimeter/\"\n",
        "        new_file_path_fraction=\"/content/gdrive/MyDrive/Colab Notebooks/Data/Variables/Cloud Fraction/\"\n",
        "        new_file_path_height=\"/content/gdrive/MyDrive/Colab Notebooks/Data/Variables/Height/\"\n",
        "        new_file_path_counter=\"/content/gdrive/MyDrive/Colab Notebooks/Data/Variables/Counter/\"\n",
        "        new_file_name_perimeter=new_file_path_perimeter+\"Perimeter_\"+filename_str[1:-5]+\".csv\"\n",
        "        new_file_name_fraction=new_file_path_fraction+\"Fraction_\"+filename_str[1:-5]+\".csv\"\n",
        "        new_file_name_height=new_file_path_height+\"Height_\"+filename_str[1:-5]+\".csv\"\n",
        "        new_file_name_counter=new_file_path_counter+\"Counter_\"+filename_str[1:-5]+\".csv\"\n",
        "        np.savetxt(new_file_name_perimeter, cloud_perimeter_vector)\n",
        "        np.savetxt(new_file_name_fraction, cloud_fraction_vector)\n",
        "        np.savetxt(new_file_name_height, height_vector)\n",
        "        np.savetxt(new_file_name_counter, perimeter_values_vector)\n",
        "\n",
        "\n",
        "        df=df.drop(index=row_index)\n",
        "        df.to_csv(file_list_csv, index=False)\n",
        "\n",
        "\n",
        "\n",
        "        print(new_file_name_perimeter)\n",
        "        print('Counter='+str(counter))\n",
        "        yield counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khKiXi9rhC90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "outputId": "cf253782-07bc-48d4-8ee7-45f5bda69c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'sgpcogspiN1.c1.20180531.141920.nc',\n",
            "/content/gdrive/MyDrive/Colab Notebooks/Data/Bath/COGS/sgpcogspiN1.c1.20180531.141920.nc\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c2ce74243415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_files_iterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_generator_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-27e29cc3ea5e>\u001b[0m in \u001b[0;36mfile_generator_padding\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloud_data_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloud_data_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloud_data_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cloud'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloud_data_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m#turn deprecation warnings back on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msrc/netCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Variable.__array__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32msrc/netCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Variable.__getitem__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32msrc/netCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Variable._toma\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_any\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "all_files_iterator=True\n",
        "for i in file_generator_padding():\n",
        "  if i>200:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L5cT3CfhFLO"
      },
      "outputs": [],
      "source": [
        "def fill(data, invalid=None):\n",
        "    \"\"\"\n",
        "    Replace the value of invalid 'data' cells (indicated by 'invalid') \n",
        "    by the value of the nearest valid data cell\n",
        "\n",
        "    Input:\n",
        "        data:    numpy array of any dimension\n",
        "        invalid: a binary array of same shape as 'data'. True cells set where data\n",
        "                 value should be replaced.\n",
        "                 If None (default), use: invalid  = np.isnan(data)\n",
        "\n",
        "    Output: \n",
        "        Return a filled array. \n",
        "    \"\"\"\n",
        "\n",
        "    if invalid is None: invalid = np.isnan(data)\n",
        "\n",
        "    ind = nd.distance_transform_edt(invalid, return_distances=False, return_indices=True)\n",
        "    return data[tuple(ind)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM8DpSf2hTMl"
      },
      "outputs": [],
      "source": [
        "# working atmospheric alignment\n",
        "def atmospheric_generator_simple():\n",
        "    # initialise a counter for the number of files considered\n",
        "    counter = 0\n",
        "    # store the path to the COGS files as a variable\n",
        "    directory = \"/content/gdrive/MyDrive/Colab Notebooks/Data/Bath/COGS/\"\n",
        "    # specify the folder containing the atmospheric files\n",
        "    atmospheric_directory = \"/content/gdrive/MyDrive/Colab Notebooks/Data/Bath/\"\n",
        "    # specify the folder containing the cloud perimeter value counter files\n",
        "    counter_directory = \"/content/gdrive/MyDrive/Colab Notebooks/Data/Variables/Counter/\"\n",
        "    # store the file path for the list of all COGS files in chronological order\n",
        "    file_list_csv = \"/content/gdrive/MyDrive/Colab Notebooks/Data/processing_file_list_after23.csv\"\n",
        "    # use pandas to read in the file list csv file and iterate through them\n",
        "    df = pd.read_csv(file_list_csv)\n",
        "    for row_index, filename in df.iterrows():\n",
        "        print(filename['File_names'])\n",
        "        # access each file name in the list using the key 'File_names'\n",
        "        filename_str = str(filename['File_names'])\n",
        "        # print the date as an error check\n",
        "        print(filename_str[16:-12])\n",
        "        # find the name of the corresponding atmospheric file\n",
        "        atmospheric_filename = 'sgpinterpolatedsondeC1.c1.' + \\\n",
        "            filename_str[16:-12]+'.000030.nc'\n",
        "        # specify the path to the atmospheric file\n",
        "        atmospheric_file_path = atmospheric_directory + atmospheric_filename\n",
        "        # print the file path to check it has been formatted correctly\n",
        "        print(atmospheric_file_path)\n",
        "        # read in the netCDF data as a Dataset\n",
        "        interpolated_data = Dataset(atmospheric_file_path, mode='r')\n",
        "        # store the path to the current cloud file as a variable\n",
        "        file_path = directory+filename_str[1:-2]\n",
        "        # read in the netCDF data as a Dataset\n",
        "        cloud_data_set = Dataset(file_path, mode='r')\n",
        "        # access the counter file from the directory and store as a dataframe\n",
        "        perimeter_counter_file_path = counter_directory + \\\n",
        "            \"Counter_\"+filename_str[1:-5]+\".csv\"\n",
        "        counter_df = pd.read_csv(perimeter_counter_file_path, header=None)\n",
        "        # then convert to numpy array of integers\n",
        "        counter_df = counter_df.to_numpy(dtype='i')\n",
        "        # add one to the counter as all new files have been accessed\n",
        "        counter += 1\n",
        "        # suppress the warnings that arise when casting as a numpy array\n",
        "        warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "        # store the different cloud variables as numpy arrays\n",
        "        #x = np.array(cloud_data_set.variables['x'])\n",
        "        #y = np.array(cloud_data_set.variables['y'])\n",
        "        #z = np.array(cloud_data_set.variables['z'])\n",
        "        #cloud = np.array(cloud_data_set.variables['cloud'])\n",
        "        cloud_time = np.array(cloud_data_set.variables['time'])\n",
        "        # cast the net cdf atmospheric variables as numpy arrays\n",
        "        base_time = np.array(interpolated_data.variables['base_time'])\n",
        "        time_offset = np.array(interpolated_data.variables['time_offset'])\n",
        "        interpolated_time = np.array(interpolated_data.variables['time'])\n",
        "        height = np.array(interpolated_data.variables['height'])\n",
        "        u_wind = np.array(interpolated_data.variables['u_wind'])\n",
        "        qc_u_wind = np.array(interpolated_data.variables['qc_u_wind'])\n",
        "        v_wind = np.array(interpolated_data.variables['v_wind'])\n",
        "        qc_v_wind = np.array(interpolated_data.variables['qc_v_wind'])\n",
        "        precip = np.array(interpolated_data.variables['precip'])\n",
        "        qc_precip = np.array(interpolated_data.variables['qc_precip'])\n",
        "        temp = np.array(interpolated_data.variables['temp'])\n",
        "        qc_temp = np.array(interpolated_data.variables['qc_temp'])\n",
        "        bar_pres = np.array(interpolated_data.variables['bar_pres'])\n",
        "        qc_bar_pres = np.array(interpolated_data.variables['qc_bar_pres'])\n",
        "        rh_scaled = np.array(interpolated_data.variables['rh_scaled'])\n",
        "        qc_rh_scaled = np.array(interpolated_data.variables['qc_rh_scaled'])\n",
        "        potential_temp = np.array(\n",
        "            interpolated_data.variables['potential_temp'])\n",
        "        qc_potential_temp = np.array(\n",
        "            interpolated_data.variables['qc_potential_temp'])\n",
        "        sh = np.array(interpolated_data.variables['sh'])\n",
        "        qc_sh = np.array(interpolated_data.variables['qc_sh'])\n",
        "        wdir = np.array(interpolated_data.variables['wdir'])\n",
        "        qc_wdir = np.array(interpolated_data.variables['qc_wdir'])\n",
        "        vap_pres = np.array(interpolated_data.variables['vap_pres'])\n",
        "        qc_vap_pres = np.array(interpolated_data.variables['qc_vap_pres'])\n",
        "        dp = np.array(interpolated_data.variables['dp'])\n",
        "        qc_dp = np.array(interpolated_data.variables['qc_dp'])\n",
        "        wspd = np.array(interpolated_data.variables['wspd'])\n",
        "        qc_wspd = np.array(interpolated_data.variables['qc_wspd'])\n",
        "        # turn deprecation warnings back on\n",
        "        warnings.filterwarnings(\"default\", category=DeprecationWarning)\n",
        "\n",
        "        '''#replace all values which are not quality with the mean of all quality readings\n",
        "    #THIS MAY NOT BE THE BEST STRATEGY BUT SHOULD ESSENTIALLY TELL THE NEURAL NET\n",
        "    #TO DO THE USUAL THING WITH THAT VARIABLE\n",
        "    wspd[qc_wspd>=1]=np.mean(wspd[qc_wspd<1])\n",
        "    u_wind[qc_u_wind>=1]=np.mean(u_wind[qc_u_wind<1])\n",
        "    v_wind[qc_v_wind>=1]=np.mean(v_wind[qc_v_wind<1])\n",
        "    temp[qc_temp>=1]=np.mean(temp[qc_temp<1])\n",
        "    bar_pres[qc_bar_pres>=1]=np.mean(bar_pres[qc_bar_pres<1])\n",
        "    rh_scaled[qc_rh_scaled>=1]=np.mean(rh_scaled[qc_rh_scaled<1])\n",
        "    potential_temp[qc_potential_temp>=1]=np.mean(potential_temp[qc_potential_temp<1])\n",
        "    sh[qc_sh>=1]=np.mean(sh[qc_sh<1])\n",
        "    wdir[qc_wdir>=1]=np.mean(wdir[qc_wdir<1])\n",
        "    vap_pres[qc_vap_pres>=1]=np.mean(vap_pres[qc_vap_pres<1])\n",
        "    dp[qc_dp>=1]=np.mean(dp[qc_dp<1])'''\n",
        "\n",
        "        #instead use the fill function defined in the previous cell to replace\n",
        "        #any invalid value with the nearest valid value\n",
        "        wspd_invalid= qc_wspd>=1\n",
        "        u_wind_invalid= qc_u_wind>=1\n",
        "        v_wind_invalid= qc_v_wind>=1\n",
        "        temp_invalid= qc_temp>=1\n",
        "        bar_pres_invalid= qc_bar_pres>=1\n",
        "        rh_scaled_invalid= qc_rh_scaled>=1\n",
        "        potential_temp_invalid= qc_potential_temp>=1\n",
        "        sh_invalid= qc_sh>=1\n",
        "        vap_pres_invalid= qc_vap_pres>=1\n",
        "        dp_invalid= qc_dp>=1\n",
        "\n",
        "        wspd = fill(wspd, invalid=wspd_invalid)\n",
        "        u_wind = fill(u_wind, invalid=u_wind_invalid)\n",
        "        v_wind = fill(v_wind, invalid=v_wind_invalid)\n",
        "        temp = fill(temp, invalid=temp_invalid)\n",
        "        bar_pres = fill(bar_pres, invalid=bar_pres_invalid)\n",
        "        rh_scaled = fill(rh_scaled, invalid=rh_scaled_invalid)\n",
        "        potential_temp = fill(potential_temp, invalid=potential_temp_invalid)\n",
        "        sh = fill(sh, invalid=sh_invalid)\n",
        "        vap_pres = fill(vap_pres, invalid=vap_pres_invalid)\n",
        "        dp = fill(dp, invalid=dp_invalid)\n",
        "\n",
        "        # normalise the variables ready for the neural network\n",
        "        rh_scaled = (rh_scaled - np.mean(rh_scaled)) / np.std(rh_scaled)\n",
        "        wspd = (wspd - np.mean(wspd)) / np.std(wspd)\n",
        "        u_wind = (u_wind - np.mean(u_wind)) / np.std(u_wind)\n",
        "        v_wind = (v_wind - np.mean(v_wind)) / np.std(v_wind)\n",
        "        temp = (temp - np.mean(temp)) / np.std(temp)\n",
        "        bar_pres = (bar_pres - np.mean(bar_pres)) / np.std(bar_pres)\n",
        "        potential_temp = (potential_temp - np.mean(potential_temp)) / np.std(potential_temp)\n",
        "        sh = (sh - np.mean(sh)) / np.std(sh)\n",
        "        vap_pres = (vap_pres - np.mean(vap_pres)) / np.std(vap_pres)\n",
        "        dp = (dp - np.mean(dp)) / np.std(dp)\n",
        "\n",
        "        # find the height dimension\n",
        "        height_dimension = np.shape(u_wind)[1]\n",
        "        atmospheric_variables_array = np.zeros((2,int(height_dimension),10))\n",
        "\n",
        "        for i in range(len(cloud_time)):\n",
        "            assert len(cloud_time) == len(\n",
        "                counter_df), \"mismatch between number of time steps in cloud_time and timesteps in the number of perimeter values vector\"\n",
        "            num_perimeter_values = int(counter_df[i])\n",
        "            index = (cloud_time[i] - int(base_time)) // 60\n",
        "            if (cloud_time[i] % 60) == 0:\n",
        "                index = index - 1\n",
        "            # initialise an np array\n",
        "            atmospheric_variables_mini_array = np.zeros((2,int(height_dimension),10))\n",
        "            atmospheric_variables = np.zeros((int(height_dimension),10))\n",
        "            atmospheric_variables[:, 0] = wspd[index, :]\n",
        "            atmospheric_variables[:, 1] = u_wind[index, :]\n",
        "            atmospheric_variables[:, 2] = v_wind[index, :]\n",
        "            atmospheric_variables[:, 3] = temp[index, :]\n",
        "            atmospheric_variables[:, 4] = bar_pres[index, :]\n",
        "            atmospheric_variables[:, 5] = rh_scaled[index, :]\n",
        "            atmospheric_variables[:, 6,] = potential_temp[index, :]\n",
        "            atmospheric_variables[:, 7] = sh[index, :]\n",
        "            atmospheric_variables[:, 8] = vap_pres[index, :]\n",
        "            atmospheric_variables[:, 9] = dp[index, :]\n",
        "            for k in range(num_perimeter_values):\n",
        "                atmospheric_variables_mini_array = np.concatenate((atmospheric_variables_mini_array,\n",
        "                         atmospheric_variables.reshape(1,atmospheric_variables.shape[0],atmospheric_variables.shape[1])), axis=0)\n",
        "            atmospheric_variables_mini_array = atmospheric_variables_mini_array[2:, :, :]\n",
        "            atmospheric_variables_array = np.concatenate((atmospheric_variables_array,\n",
        "                         atmospheric_variables_mini_array), axis=0)\n",
        "        atmospheric_variables_array = atmospheric_variables_array[2:, :, :]\n",
        "\n",
        "\n",
        "        new_file_path_atmospheric = \"/content/gdrive/MyDrive/Colab Notebooks/Data/Variables/Atmospheric/\"\n",
        "        new_file_name_atmospheric = new_file_path_atmospheric + \\\n",
        "            \"Atmospheric_\"+filename_str[1:-5]+\".npy\"\n",
        "        print(atmospheric_variables_array.shape)\n",
        "        print(int(len(cloud_time))*num_perimeter_values)\n",
        "\n",
        "        #alternative save to csv\n",
        "        '''#reshape the array from a 3D matrix to a 2D matrix\n",
        "        atmospheric_variables_array_reshaped = atmospheric_variables_array.reshape(atmospheric_variables_array.shape[0], -1)\n",
        "\n",
        "        #save reshaped array to file\n",
        "        np.savetxt(new_file_name_atmospheric, atmospheric_variables_array_reshaped)\n",
        "        \n",
        "        #error check\n",
        "        if (index==0 or index==1):\n",
        "              #retrieving data from file.\n",
        "              loaded_arr = np.loadtxt(new_file_name_atmospheric)\n",
        "\n",
        "              #The loaded array is a 2D array, therefore we need to convert it to the original\n",
        "              #array shape to get original matrix with original shape.\n",
        "              load_original_arr = loaded_arr.reshape(loaded_arr.shape[0], loaded_arr.shape[1] // atmospheric_variables_array.shape[2], atmospheric_variables_array.shape[2])\n",
        "\n",
        "              # check the shapes:\n",
        "              print(\"shape of arr: \", atmospheric_variables_array.shape)\n",
        "              print(\"shape of load_original_arr: \", load_original_arr.shape)\n",
        "\n",
        "              # check if both arrays are same or not:\n",
        "              if (load_original_arr == atmospheric_variables_array).all():\n",
        "                print(\"Yes, both the arrays are same\")\n",
        "              else:\n",
        "                print(\"No, both the arrays are not same\")\n",
        "        '''\n",
        "\n",
        "\n",
        "        atmospheric_variables_array = atmospheric_variables_array.astype(np.float16)\n",
        "        np.save(new_file_name_atmospheric, atmospheric_variables_array)\n",
        "\n",
        "        df=df.drop(index=row_index)\n",
        "        df.to_csv(file_list_csv)\n",
        "\n",
        "        print(new_file_name_atmospheric)\n",
        "        print('Counter='+str(counter))\n",
        "        yield counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "EQ69fxmJh3Vx",
        "outputId": "431418a3-6748-4959-f6e8-126722aa151e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'sgpcogspiN1.c1.20180704.190000.nc',\n",
            "20180704\n",
            "/content/gdrive/MyDrive/Colab Notebooks/Data/Bath/sgpinterpolatedsondeC1.c1.20180704.000030.nc\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-37e01bfa3d8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0matmospheric_files_iterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0matmospheric_generator_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-e5e8d64f3da7>\u001b[0m in \u001b[0;36matmospheric_generator_simple\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0matmospheric_variables_mini_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matmospheric_variables_mini_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             atmospheric_variables_array = np.concatenate((atmospheric_variables_array,\n\u001b[0;32m--> 162\u001b[0;31m                          atmospheric_variables_mini_array), axis=0)\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0matmospheric_variables_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matmospheric_variables_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "atmospheric_files_iterator=True\n",
        "for i in atmospheric_generator_simple():\n",
        "  if i>200:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22xymVZ2ldjX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}